\section{Outlook}
The implementation discussed in this paper focused on scheduling and placement of Flink jobs with
knowledge about the physical network. Other issues and enhancements remain open to take complete
advantage of network aware job scheduling.

\subsection{Traffic Properties}
Monitoring would enable better scheduling and flow programming by taking latency and network
utilization into account. The OpenFlow specification \cite{openflow} offers port and flow based
packet and byte counters, but neither measures latency, nor bandwidth. Therefore monitoring has to
be somehow implemented.

One approach to determine latency between two switches is to insert additional OpenFlow packets and
measure the duration.  \texttt{PACKET OUT} messages forward additional packages to one or more
ports.  The OpenFlow controller receives a \texttt{PACKET IN} message when a switch could not assign
a packet to a flow of its flow table. The latency can then be easily calculated by subtracting the
time of packet creation and roundtrip times from the time the packet was received via \texttt{PACKET
IN}. \cite{monitoringlatency} \cite{opennetmon}

OpenFlow also supports a \texttt{FLOW REMOVED} message that is received by the controller when a
flow entry of a switch is expired. This message contains information about the duration an entry
was active in the flow table, the amount of traffic matched against an entry and the input port to
determine the source of traffic.  These information enable the computation of utilization between
inter-switch links. This approach does not produce any instrumentation overhead since the
\texttt{FLOW REMOVED} messages are send anyhow. \cite{flowsense}

Another way to measure network utilization is to poll the OpenFlow statistics. The problem is to
find an intelligent polling algorithm to reduce the load on switches. Using the routing information
of a flow is helpful to find an appropriate querying strategy.  A good compromise between accuracy
and load balancing is to query two switches randomly and select the one closer to the destination.
\cite{opentm} \cite{opennetmon}

There are implementations for the OpenFlow Controller POX \cite{opennetmon}, NOX \cite{opentm} and
Floodlight \cite{flowsense}.

\subsection{Load Balancing}
Even using the Bottom-Up approach, the possibility to control data flows in the network still remains.
The notion of balancing data flows of alike source and sink around multiple paths comes to mind.
OpenDaylights' default flow configuration reacts upon activation by a switch clueless to an incoming
packets' destination. The controller then prompts the updating of all switches connected to the controller
so that future packets to said destination host will follow along the shortest path.

This behavior might be sufficient in the case where sources and sinks are not grouped around common
switches but in our case we would have lots of tasks deployed on host groups characterized by single
switch connections due to our scheduling decisions. In the assumed case of all-to-all data-flows
between two or more host groups, we would not want to send all data along a single path. Even with
the switch of each host group posing as a natural bottleneck, we assume better overall load
balancing would stem from using more than a single shortest path which we could determine with Yen's
Algorithm \cite{yens}.